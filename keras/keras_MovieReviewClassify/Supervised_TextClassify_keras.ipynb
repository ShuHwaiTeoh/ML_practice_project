{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from https://www.youtube.com/watch?v=6g4O5UOH304&t=3909s\n",
    "# classify that whether the review thinks the movie is good or bad\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data: \n",
      " [1, 591, 202, 14, 31, 6, 717, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 7944, 451, 202, 14, 6, 717]\n",
      "origin review: \n",
      " <START> please give this one a miss br br <UNK> <UNK> and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite <UNK> so all you madison fans give this a miss\n"
     ]
    }
   ],
   "source": [
    "# imdb: Dataset of 25,000 movies reviews\n",
    "data = keras.datasets.imdb\n",
    "# use the data encoded by 10000-words tokenizer: choose 10000 most frequent words from the sentences to tokenize\n",
    "(train_data, train_label), (test_data, test_label) = data.load_data(num_words=10000)\n",
    "\n",
    "# returns a dictionary of key value pairs where \n",
    "# the key is the word in the sentence and the value is the label assigned to it.\n",
    "word_index = data.get_word_index()\n",
    "\n",
    "# reconstruct the dictionary to recover the original plaintext reviews\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "# v+3 to spare the value for padding, start, unknown, and unused\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "# To recover the encoded data to plaintext\n",
    "reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])\n",
    "plaintext = \" \".join([reverse_word_index.get(i, '?') for i in test_data[0]])\n",
    "print(\"test_data: \\n\", test_data[0])\n",
    "print(\"origin review: \\n\", plaintext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing data\n",
    "# padding at the end of sequences that length is shorter than 300\n",
    "# extraxt 300 words from the beginning of the sequences that lenths are longer than 300\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value=word_index[\"<PAD>\"], \n",
    "                                                        padding=\"post\", maxlen=256)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value=word_index[\"<PAD>\"], \n",
    "                                                        padding=\"post\", maxlen=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid or Logistic Activation Function\n",
    "    - The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n",
    "    - The softmax function is a more generalized logistic activation function which is used for multiclass classification.\n",
    "### Tanh or hyperbolic tangent Activation Function\n",
    "    - The range of the tanh function is from (-1 to 1). The tanh function is mainly used classification between two classes.\n",
    "### ReLU (Rectified Linear Unit) Activation Function: Range: [ 0 to infinity)\n",
    "    -The biggest advantage of ReLu is indeed non-saturation of its gradient, which greatly accelerates the convergence of stochastic gradient descent compared to the sigmoid / tanh functions (paper by Krizhevsky et al).\n",
    "    -Because of the horizontal line in ReLu( for negative X ), the gradient can go towards 0.\n",
    "### Leaky ReLU: range of the Leaky ReLU is (-infinity to infinity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
    "![title](activationFunction.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model = keras.Sequential()\n",
    "# embedding layers group similar words to gether (word vectors that have small differences in degrees)\n",
    "# represent each word in 1*16 word vector: [c1, c2, ..., c16] = a certain word\n",
    "model.add(keras.layers.Embedding(10000,16)) # 10000 num_words = 10000 word vectors in 16 dimension\n",
    "model.add(keras.layers.GlobalAveragePooling1D()) # average the value for each word vector\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "# sigmoid let the result between 0~1 to show the probabilities of 0:bad or 1:good\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss function: https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
    "1. Regression Loss Functions\n",
    "    - Mean Squared Error Loss\n",
    "    - Mean Squared Logarithmic Error Loss\n",
    "    - Mean Absolute Error Loss\n",
    "2. Binary Classification Loss Functions\n",
    "    - Binary Cross-Entropy\n",
    "    - Hinge Loss\n",
    "    - Squared Hinge Loss\n",
    "3. Multi-Class Classification Loss Functions\n",
    "    - Multi-Class Cross-Entropy Loss\n",
    "    - Sparse Multiclass Cross-Entropy Loss\n",
    "    - Kullback Leibler Divergence Loss\n",
    "    \n",
    "#### The batch size is a number of samples processed before the model is updated. Epoch is the number of complete passes through the training dataset. The size of a batch must be more than or equal to one and less than or equal to the number of samples in the training dataset.\n",
    "\n",
    "### Protocol of building a solid model: \n",
    "- splitting your data into three sets: One for training, one for validation and one for final evalution, which is the test set.\n",
    "- you train on your training data and tune your model with the results of metrics (accuracy, loss etc) that you get from your validation set.\n",
    "- Your model doesn't \"see\" your validation set and isnÂ´t in any way trained on it, but you as the architect and master of the hyperparameters tune the model according to this data. Therefore it indirectly influences your model because it directly influences your design decisions. You nudge your model to work well with the validation data and that can possibly bring in a tilt.\n",
    "\n",
    "#### verbose 0 (silent), 1 (progress bar) or 2 (number) you just say how do you want to 'see' the training progress for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 2s 97us/sample - loss: 0.3707 - accuracy: 0.8678\n",
      "Loss:  0.370737602519989  / Accuracy:  0.86784\n",
      "\n",
      "\n",
      "Review: \n",
      " <START> please give this one a miss br br <UNK> <UNK> and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite <UNK> so all you madison fans give this a miss <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Prediction:  [0.04823887]\n",
      "Actual:  0\n",
      "\n",
      "\n",
      "Review: \n",
      " a lot of patience because it focuses on mood and character development the plot is very simple and many of the scenes take place on the same set in frances <UNK> the sandy dennis character apartment but the film builds to a disturbing climax br br the characters create an atmosphere <UNK> with sexual tension and psychological <UNK> it's very interesting that robert altman directed this considering the style and structure of his other films still the trademark altman audio style is evident here and there i think what really makes this film work is the brilliant performance by sandy dennis it's definitely one of her darker characters but she plays it so perfectly and convincingly that it's scary michael burns does a good job as the mute young man regular altman player michael murphy has a small part the <UNK> moody set fits the content of the story very well in short this movie is a powerful study of loneliness sexual <UNK> and desperation be patient <UNK> up the atmosphere and pay attention to the wonderfully written script br br i praise robert altman this is one of his many films that deals with unconventional fascinating subject matter this film is disturbing but it's sincere and it's sure to <UNK> a strong emotional response from the viewer if you want to see an unusual film some might even say bizarre this is worth the time br br unfortunately it's very difficult to find in video stores you may have to buy it off the internet\n",
      "Prediction:  [0.9996773]\n",
      "Actual:  1\n",
      "\n",
      "\n",
      "Review: \n",
      " no improvement and demand a different king irritated <UNK> sends them a <UNK> br br delighted with this <UNK> looking new king who towers above them the <UNK> welcome him with a <UNK> of <UNK> dressed <UNK> the mayor steps forward to hand him the key to the <UNK> as <UNK> cameras record the event to everyone's horror the <UNK> promptly eats the mayor and then goes on a merry rampage <UNK> citizens at random a title card <UNK> reads news of the king's <UNK> throughout the kingdom when the now terrified <UNK> once more <UNK> <UNK> for help he loses his temper and <UNK> their community with lightning <UNK> the moral of our story delivered by a hapless frog just before he is eaten is let well enough alone br br considering the time period when this startling little film was made and considering the fact that it was made by a russian <UNK> at the height of that <UNK> country's civil war it would be easy to see this as a <UNK> about those events <UNK> may or may not have had <UNK> turmoil in mind when he made <UNK> but whatever <UNK> his choice of material the film stands as a <UNK> tale of universal <UNK> <UNK> could be the soviet union italy germany or japan in the 1930s or any country of any era that lets its guard down and is overwhelmed by <UNK> it's a fascinating film even a charming one in its macabre way but its message is no joke\n",
      "Prediction:  [0.7081765]\n",
      "Actual:  1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "# divide training data into training dataset and validation dataset\n",
    "x_val, y_val = train_data[:10000], train_label[:10000]\n",
    "x_train, y_train = train_data[10000:], train_label[10000:]\n",
    "fitModel = model.fit(x_train,y_train, epochs=50, batch_size=512, \n",
    "                     validation_data=(x_val,y_val), verbose=0)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_data, test_label)\n",
    "print(\"Loss: \", loss, \" / Accuracy: \", accuracy)\n",
    "print()\n",
    "\n",
    "# print(test_data.shape)\n",
    "prediction = model.predict(test_data)\n",
    "for i in range(3):\n",
    "    print()\n",
    "    print(\"Review: \\n\", \" \".join([reverse_word_index.get(i, '?') for i in test_data[i]]))\n",
    "    print(\"Prediction: \", str(prediction[i]))\n",
    "    print(\"Actual: \", str(test_label[i]))\n",
    "    print()\n",
    "\n",
    "# save trained model\n",
    "model.save(\"Movie_Review_Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: \n",
      " Fabulous movie. We saw it on the Philippines on the 20th and it made us want to leave our tropical paradise for snow covered mountains. Great to look at and a movie for the whole family from the young uns to grandparents. A must see for families. Probably not one for jaded snowflakes without kids\n",
      "eEncode: \n",
      " [[1657 1657   13 3363  963  590    2 1604 3363    2  830 2023  963    2\n",
      "   241 2241  830 2023    2    6 3363 1095    2   13  830    2 1983    6\n",
      "  1095  963    2 1206  590    2 1992    6 3363  830    2  830 1604    2\n",
      "  2014  963    6 1964  963    2 1604 1206 1479    2  830 1479 1604 1657\n",
      "    13 1148    6 2014    2 1657    6 1479    6 1095   13  590  963    2\n",
      "  1209 1604 1479    2  590 3363 1604 1992    2 1148 1604 1964  963 1479\n",
      "   963 1095    2 1983 1604 1206 3363  830    6   13 3363  590    2 1331\n",
      "  1479  963    6  830    2  830 1604    2 2014 1604 1604 2295    2    6\n",
      "   830    2    6 3363 1095    2    6    2 1983 1604 1964   13  963    2\n",
      "  1209 1604 1479    2  830 2023  963    2 1992 2023 1604 2014  963    2\n",
      "  1209    6 1983   13 2014 5135    2 1209 1479 1604 1983    2  830 2023\n",
      "   963    2 5135 1604 1206 3363 1331    2 1206 3363  590    2  830 1604\n",
      "     2 1331 1479    6 3363 1095 1657    6 1479  963 3363  830  590    2\n",
      "     6    2 1983 1206  590  830    2  590  963  963    2 1209 1604 1479\n",
      "     2 1209    6 1983   13 2014   13  963  590    2 1657 1479 1604  503\n",
      "     6  503 2014 5135    2 3363 1604  830    2 1604 3363  963    2 1209\n",
      "  1604 1479    2 1468    6 1095  963 1095    2  590 3363 1604 1992 1209\n",
      "  2014    6 2295  963  590    2 1992   13  830 2023 1604 1206  830    2\n",
      "  2295   13 1095  590]]\n",
      "[1.626649e-05]\n"
     ]
    }
   ],
   "source": [
    "trained_model = keras.models.load_model(\"Movie_Review_Model.h5\")\n",
    "\n",
    "import re \n",
    "def multiple_replace(dict, text):\n",
    "  # Create a regular expression  from the dictionary keys\n",
    "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "  # For each match, look-up corresponding value in dictionary\n",
    "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text) \n",
    "\n",
    "def review_encode(s):\n",
    "    encoded = [1]\n",
    "    for word in s:\n",
    "        if word.lower() in word_index:\n",
    "            encoded.append(word_index[word.lower()])\n",
    "        else:\n",
    "            encoded.append(2)\n",
    "    return encoded\n",
    "\n",
    "replace_dict = {\",\":\"\", \".\":\"\", \"(\":\"\", \")\":\"\", \":\":\"\", \"\\\"\":\"\"}\n",
    "with open(\"five_star_review.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        cleaned_line = multiple_replace(replace_dict, line)\n",
    "        encode = review_encode(cleaned_line)\n",
    "        encode = keras.preprocessing.sequence.pad_sequences([encode], value=word_index[\"<PAD>\"], \n",
    "                                                            padding=\"post\", maxlen=256)\n",
    "        predict = trained_model.predict(encode)\n",
    "        print(\"Review: \\n\", line)\n",
    "        print(\"eEncode: \\n\", encode)\n",
    "        print(predict[0])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
